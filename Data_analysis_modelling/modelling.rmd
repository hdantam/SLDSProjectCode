---
title: "resale_modelling"
author: "Hemant Dantam"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
rm(list = ls())
library(dplyr)
library(stringr)
library(ggplot2)
library(xgboost)
```

```{r}
resale <- read.csv("resale_modelling.csv")
str(resale)
sort(table(resale$flat_model))

#Lets merge the less popular ones so they don't mess with regression 
resale <- resale %>%
  mutate(
    flat_model_group = case_when(
      # 1. All maisonette designs → one "Maisonette" group
      flat_model %in% c("Maisonette",
                        "Premium Maisonette",
                        "Improved-Maisonette",
                        "Model A-Maisonette") ~ "Maisonette",
      
      # 2. Rare / special types → one "Special / Rare Types" group
      flat_model %in% c("Terrace",
                        "Premium Apartment Loft",
                        "3Gen",
                        "Multi Generation",
                        "Adjoined flat",
                        "Type S1",
                        "Type S2") ~ "Special / Rare Types",
      
      # 3. Everything else: keep original label
      TRUE ~ as.character(flat_model)
    ),
    flat_model_group = factor(flat_model_group)
  )

#Cleaning the data to use for testing
resale$flat_model <- resale$flat_model_group
resale$flat_model_group <- NULL

sort(table(resale$flat_model))
```

```{r}
#Convert categorical values to factors
resale$flat_type <- as.factor(resale$flat_type)
resale$town <- as.factor(resale$town)
resale$resale_price_real <- NULL
```


Starting linear regression
```{r}
train_data <- resale %>% filter(resale_year < 2025)
test_data  <- resale %>% filter(resale_year == 2025)

factor_vars <- c("town", "flat_type", "flat_model")

for (v in factor_vars) {
  test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
}
```

Without coordinate/distances
```{r}
#LM without coordinates/distances
lm_no_gis <- lm(
  resale_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    resale_year + storey_mid,
  data = train_data
)

summary(lm_no_gis)
```
```{r}
#Testing
test_data$pred_no_gis <- predict(lm_no_gis, newdata = test_data)
```

```{r}
#With GIS
lm_with_gis <- lm(
  resale_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    closest_school_dist + closest_mrt_dist + closest_bus_dist +
    resale_year + storey_mid + X + Y,
  data = train_data
)

test_data$pred_with_gis <- predict(lm_with_gis, newdata = test_data)
summary(lm_with_gis)
```
```{r}
install.packages("Metrics")
library(Metrics)
# RMSE and MAE
rmse_no_gis   <- rmse(test_data$resale_price, test_data$pred_no_gis)
rmse_with_gis <- rmse(test_data$resale_price, test_data$pred_with_gis)

mae_no_gis   <- mae(test_data$resale_price, test_data$pred_no_gis)
mae_with_gis <- mae(test_data$resale_price, test_data$pred_with_gis)

# R-squared
r2_calc <- function(actual, predicted) {
  rss <- sum((actual - predicted)^2)
  tss <- sum((actual - mean(actual))^2)
  1 - rss/tss
}

r2_no_gis   <- r2_calc(test_data$resale_price, test_data$pred_no_gis)
r2_with_gis <- r2_calc(test_data$resale_price, test_data$pred_with_gis)

data.frame(
  Model = c("Linear Regression (No GIS)", "Linear Regression (With GIS)"),
  RMSE_2025 = c(rmse_no_gis, rmse_with_gis),
  MAE_2025 = c(mae_no_gis, mae_with_gis),
  R2_2025  = c(r2_no_gis, r2_with_gis)
)
```
```{r}
#Useful plots
par(mfrow = c(2,2))
plot(lm_with_gis)
```

```{r}
#Predicted vs actual
ggplot(test_data, aes(x = resale_price, y = pred_with_gis)) +
  geom_point(alpha = 0.3) +
  geom_abline(color = "red") +
  labs(title = "Predicted vs Actual (2025 Data) — Linear Regression")
```
Doing the same for log price
```{r}
#Using log price
resale$log_price <- log(resale$resale_price)

train_data <- resale %>% filter(resale_year < 2025)
test_data  <- resale %>% filter(resale_year == 2025)

factor_vars <- c("town", "flat_type", "flat_model")
for (v in factor_vars) {
  test_data[[v]] <- factor(test_data[[v]], levels = levels(train_data[[v]]))
}

lm_no_gis_log <- lm(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    resale_year + storey_mid,
  data = train_data
)
summary(lm_no_gis_log)
```
```{r}
#Modelling
test_data$pred_no_gis_log <- exp(predict(lm_no_gis_log, newdata = test_data))

lm_with_gis_log <- lm(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    closest_school_dist + closest_mrt_dist + closest_bus_dist +
    resale_year + storey_mid + X + Y,
  data = train_data
)

summary(lm_with_gis_log)
test_data$pred_with_gis_log <- exp(predict(lm_with_gis_log, newdata = test_data))
```
```{r}
#Printing output tables for RMSE/R2/MAE 
rmse_no_gis_log    <- rmse(test_data$resale_price, test_data$pred_no_gis_log)
rmse_with_gis_log  <- rmse(test_data$resale_price, test_data$pred_with_gis_log)

mae_no_gis_log     <- mae(test_data$resale_price, test_data$pred_no_gis_log)
mae_with_gis_log   <- mae(test_data$resale_price, test_data$pred_with_gis_log)

r2_calc <- function(actual, predicted) {
  rss <- sum((actual - predicted)^2)
  tss <- sum((actual - mean(actual))^2)
  1 - rss/tss
}

r2_no_gis_log    <- r2_calc(test_data$resale_price, test_data$pred_no_gis_log)
r2_with_gis_log  <- r2_calc(test_data$resale_price, test_data$pred_with_gis_log)

data.frame(
  Model = c("LR (log-price, No GIS)", "LR (log-price, With GIS)"),
  RMSE_2025 = c(rmse_no_gis_log, rmse_with_gis_log),
  MAE_2025 = c(mae_no_gis_log, mae_with_gis_log),
  R2_2025  = c(r2_no_gis_log, r2_with_gis_log)
)
```

```{r}
plot(lm_with_gis_log, which = 1)
```

```{r}
ggplot(test_data, aes(x = resale_price, y = pred_with_gis_log)) +
  geom_point(alpha = 0.3) +
  geom_abline(color = "red") +
  labs(title = "Predicted vs Actual (2025) — LR with log-price",
       x = "Actual Price",
       y = "Predicted Price") +
  theme_minimal()
```
```{r}
coef_table <- summary(lm_with_gis_log)$coefficients

coef_df <- as.data.frame(coef_table)
coef_df$Variable <- rownames(coef_df)
rownames(coef_df) <- NULL

# Reorder columns
coef_df <- coef_df %>%
  select(Variable, Estimate = Estimate, Std_Error = `Std. Error`,
         t_value = `t value`, p_value = `Pr(>|t|)`)

coef_df
```

```{r}
anova(lm_no_gis_log, lm_with_gis_log)
```

Doing XGBoost
```{r}
# Model WITHOUT GIS
x_train_no_gis <- model.matrix(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    resale_year + storey_mid,
  data = train_data
)[, -1]  # remove intercept

x_test_no_gis <- model.matrix(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    resale_year + storey_mid,
  data = test_data
)[, -1]

# Model WITH GIS
x_train_with_gis <- model.matrix(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    closest_school_dist + closest_mrt_dist + closest_bus_dist +
    resale_year + storey_mid + X + Y,
  data = train_data
)[, -1]

x_test_with_gis <- model.matrix(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    closest_school_dist + closest_mrt_dist + closest_bus_dist +
    resale_year + storey_mid + X + Y,
  data = test_data
)[, -1]
```

```{r}
y_train <- train_data$log_price
y_test  <- test_data$log_price
```

```{r}
library(xgboost)

dtrain_no_gis <- xgb.DMatrix(data = x_train_no_gis, label = y_train)
dtest_no_gis  <- xgb.DMatrix(data = x_test_no_gis,  label = y_test)

dtrain_with_gis <- xgb.DMatrix(data = x_train_with_gis, label = y_train)
dtest_with_gis  <- xgb.DMatrix(data = x_test_with_gis,  label = y_test)
```

```{r}
params <- list(
  objective = "reg:squarederror",
  eta = 0.1,              # learning rate
  max_depth = 6,          # depth of trees
  subsample = 0.8,        # row sampling
  colsample_bytree = 0.8  # feature sampling
)
nrounds <- 400  # enough trees without overfitting too badly
```

```{r}
xgb_no_gis <- xgb.train(
  params = params,
  data = dtrain_no_gis,
  nrounds = nrounds,
  watchlist = list(train = dtrain_no_gis),
  verbose = 0
)

xgb_with_gis <- xgb.train(
  params = params,
  data = dtrain_with_gis,
  nrounds = nrounds,
  watchlist = list(train = dtrain_with_gis),
  verbose = 0
)
```
```{r}
test_data$pred_xgb_no_gis   <- exp(predict(xgb_no_gis, dtest_no_gis))
test_data$pred_xgb_with_gis <- exp(predict(xgb_with_gis, dtest_with_gis))

rmse_xgb_no_gis   <- rmse(test_data$resale_price, test_data$pred_xgb_no_gis)
rmse_xgb_with_gis <- rmse(test_data$resale_price, test_data$pred_xgb_with_gis)

mae_xgb_no_gis    <- mae(test_data$resale_price, test_data$pred_xgb_no_gis)
mae_xgb_with_gis  <- mae(test_data$resale_price, test_data$pred_xgb_with_gis)

r2_calc <- function(actual, predicted) {
  rss <- sum((actual - predicted)^2)
  tss <- sum((actual - mean(actual))^2)
  1 - rss/tss
}

r2_xgb_no_gis   <- r2_calc(test_data$resale_price, test_data$pred_xgb_no_gis)
r2_xgb_with_gis <- r2_calc(test_data$resale_price, test_data$pred_xgb_with_gis)
```

```{r}
data.frame(
  Model = c("XGBoost (No GIS)", "XGBoost (With GIS)"),
  RMSE_2025 = c(rmse_xgb_no_gis, rmse_xgb_with_gis),
  MAE_2025 = c(mae_xgb_no_gis, mae_xgb_with_gis),
  R2_2025 = c(r2_xgb_no_gis, r2_xgb_with_gis)
)
```

```{r}
importance_matrix <- xgb.importance(
  model = xgb_with_gis,
  feature_names = colnames(x_train_with_gis)
)

xgb.plot.importance(importance_matrix, top_n = 20)
```
Optimization
```{r}
library(xgboost)
library(dplyr)
library(Metrics)

### 1. Prepare training folds (time-based CV on 2020–2024)

train_data$fold <- case_when(
  train_data$resale_year == 2020 ~ "fold1",
  train_data$resale_year == 2021 ~ "fold2",
  train_data$resale_year == 2022 ~ "fold3",
  train_data$resale_year == 2023 ~ "fold4",
  train_data$resale_year == 2024 ~ "fold5",
  TRUE ~ "train"   # 2017–2019
)

# Define a simple but useful tuning grid

grid <- expand.grid(
  eta = c(0.05, 0.1),         # learning rate
  max_depth = c(4, 6),        # tree depth
  subsample = c(0.8),         # row sampling
  colsample_bytree = c(0.8),  # feature sampling
  nrounds = c(300, 500),      # number of trees
  KEEP.OUT.ATTRS = FALSE,
  stringsAsFactors = FALSE
)

# Helper: CV score for ONE row of the grid (by index)

cv_score <- function(i) {
  params_row <- grid[i, ]

  fold_rmse <- c()

  for (fold in paste0("fold", 1:5)) {

    # Validation = that fold's year; training = all earlier years
    val_idx <- which(train_data$fold == fold)
    val_year <- train_data$resale_year[val_idx][1]
    train_idx <- which(train_data$resale_year < val_year)

    # Design matrices (WITH GIS)
    X_train <- model.matrix(
      log_price ~ town + flat_type + flat_model +
        floor_area_sqm + remaining_lease_years +
        closest_school_dist + closest_mrt_dist + closest_bus_dist +
        resale_year + storey_mid + X + Y,
      data = train_data[train_idx, ]
    )[ , -1]

    y_train <- train_data$log_price[train_idx]

    X_val <- model.matrix(
      log_price ~ town + flat_type + flat_model +
        floor_area_sqm + remaining_lease_years +
        closest_school_dist + closest_mrt_dist + closest_bus_dist +
        resale_year + storey_mid + X + Y,
      data = train_data[val_idx, ]
    )[ , -1]

    y_val <- train_data$log_price[val_idx]

    dtrain <- xgb.DMatrix(X_train, label = y_train)
    dval   <- xgb.DMatrix(X_val,   label = y_val)

    model <- xgb.train(
      params = list(
        objective = "reg:squarederror",
        eta = params_row$eta,
        max_depth = params_row$max_depth,
        subsample = params_row$subsample,
        colsample_bytree = params_row$colsample_bytree
      ),
      data = dtrain,
      nrounds = params_row$nrounds,
      verbose = 0
    )

    # Predict (log → exp back to dollars)
    preds  <- exp(predict(model, newdata = dval))
    actual <- exp(y_val)

    fold_rmse <- c(fold_rmse, rmse(actual, preds))
  }

  mean(fold_rmse)
}

# Run tuning over the grid

grid$rmse <- sapply(seq_len(nrow(grid)), cv_score)

best_params <- grid[which.min(grid$rmse), ]
print(best_params)

# Train final XGBoost on full 2017–2024 with best params

X_train_full <- model.matrix(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    closest_school_dist + closest_mrt_dist + closest_bus_dist +
    resale_year + storey_mid + X + Y,
  data = train_data
)[ , -1]

y_train_full <- train_data$log_price

X_test <- model.matrix(
  log_price ~ town + flat_type + flat_model +
    floor_area_sqm + remaining_lease_years +
    closest_school_dist + closest_mrt_dist + closest_bus_dist +
    resale_year + storey_mid + X + Y,
  data = test_data
)[ , -1]

dtrain_full <- xgb.DMatrix(X_train_full, label = y_train_full)
dtest       <- xgb.DMatrix(X_test,       label = test_data$log_price)

final_xgb <- xgb.train(
  params = list(
    objective = "reg:squarederror",
    eta = best_params$eta,
    max_depth = best_params$max_depth,
    subsample = best_params$subsample,
    colsample_bytree = best_params$colsample_bytree
  ),
  data = dtrain_full,
  nrounds = best_params$nrounds,
  verbose = 1
)

# Predict on 2025 + metrics

test_data$pred_xgb_tuned <- exp(predict(final_xgb, dtest))

rmse_xgb_tuned <- rmse(test_data$resale_price, test_data$pred_xgb_tuned)
mae_xgb_tuned  <- mae(test_data$resale_price, test_data$pred_xgb_tuned)
r2_xgb_tuned   <- 1 - sum((test_data$resale_price - test_data$pred_xgb_tuned)^2) /
                       sum((test_data$resale_price - mean(test_data$resale_price))^2)

results_xgb <- data.frame(
  Model    = "XGBoost (tuned with GIS)",
  RMSE_2025 = rmse_xgb_tuned,
  MAE_2025  = mae_xgb_tuned,
  R2_2025   = r2_xgb_tuned
)

print(results_xgb)
```
```{r}
library(ggplot2)
# Residuals vs Predicted (Fitted)

test_data$residuals_tuned <- test_data$resale_price - test_data$pred_xgb_tuned

ggplot(test_data, aes(x = pred_xgb_tuned, y = residuals_tuned)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Residuals vs Predicted (Tuned XGBoost)",
    x = "Predicted Resale Price (SGD)",
    y = "Residuals"
  ) +
  theme_minimal(base_size = 14)

# Actual vs Predicted

ggplot(test_data, aes(x = resale_price, y = pred_xgb_tuned)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    title = "Actual vs Predicted Resale Price (Tuned XGBoost)",
    x = "Actual Resale Price (SGD)",
    y = "Predicted Resale Price (SGD)"
  ) +
  theme_minimal(base_size = 14)
```

```{r}
importance_matrix <- xgb.importance(
  model = final_xgb,
  feature_names = colnames(X_train_full)
)

xgb.plot.importance(importance_matrix, top_n = 20)
```

```{r}
library(mgcv)
sapply(train_data[, c("floor_area_sqm",
                      "remaining_lease_years",
                      "closest_mrt_dist",
                      "closest_school_dist",
                      "closest_bus_dist",
                      "storey_mid",
                      "resale_year")],
       function(x) length(unique(x)))


gam_model <- gam(
  log_price ~ 
    town + flat_type + flat_model +
    s(floor_area_sqm, k = 20) +
    s(remaining_lease_years, k = 20) +
    s(closest_mrt_dist, k = 20) +
    s(closest_school_dist, k = 20) +
    s(closest_bus_dist, k = 20) +
    s(storey_mid, k = 6) +        # only 17 unique values
    s(resale_year, k = 4),        # only 8 unique values
  data = train_data,
  method = "REML"
)

summary(gam_model)
```


```{r}
test_data$pred_gam_log <- predict(gam_model, newdata = test_data)
test_data$pred_gam <- exp(test_data$pred_gam_log)
```

```{r}
rmse_gam <- rmse(test_data$resale_price, test_data$pred_gam)
mae_gam  <- mae(test_data$resale_price, test_data$pred_gam)
r2_gam   <- 1 - sum((test_data$resale_price - test_data$pred_gam)^2) /
                 sum((test_data$resale_price - mean(test_data$resale_price))^2)

data.frame(
  Model = "GAM",
  RMSE_2025 = rmse_gam,
  MAE_2025  = mae_gam,
  R2_2025   = r2_gam
)
```


```{r}
ggplot(test_data, aes(x = resale_price, y = pred_gam)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Actual vs Predicted (GAM)",
    x = "Actual Price (SGD)",
    y = "Predicted Price (SGD)"
  ) +
  theme_minimal(base_size = 14)
```

```{r}
test_data$residuals_gam <- test_data$resale_price - test_data$pred_gam

ggplot(test_data, aes(x = pred_gam, y = residuals_gam)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted (GAM)",
    x = "Predicted Price",
    y = "Residuals"
  ) +
  theme_minimal(base_size = 14)
```

```{r}
par(mfrow = c(3, 3))
plot(gam_model, shade = TRUE, pages = 1)
```
```{r}
final_results <- data.frame(
  Model = c(
    "Linear Regression (No GIS)",
    "Linear Regression (With GIS)",
    "GAM",
    "XGBoost (Default)",
    "XGBoost (Tuned)"
  ),
  
  RMSE_2025 = c(
    rmse_no_gis_log,     # replace with your number
    rmse_with_gis_log,   # replace with your number
    rmse_gam,           # GAM rmse
    rmse_xgb_no_gis,   # default xgboost
    rmse_xgb_tuned      # tuned xgboost
  ),
  
  MAE_2025 = c(
    mae_no_gis_log,
    mae_with_gis_log,
    mae_gam,
    mae_xgb_no_gis,
    mae_xgb_tuned
  ),
  
  R2_2025 = c(
    r2_no_gis_log,
    r2_with_gis_log,
    r2_gam,
    r2_xgb_no_gis,
    r2_xgb_tuned
  )
)

final_results
```
```{r}
library(mgcv)

gam_smaller_k <- gam(
  log_price ~ 
    town + flat_type + flat_model +
    s(floor_area_sqm, k = 10) +
    s(remaining_lease_years, k = 10) +
    s(closest_mrt_dist, k = 8) +
    s(closest_school_dist, k = 8) +
    s(closest_bus_dist, k = 8) +
    s(storey_mid, k = 6) +
    s(resale_year, k = 4),
  data = train_data,
  method = "REML"
)

summary(gam_smaller_k)
```
```{r}
par(mfrow = c(3, 3))
plot(gam_smaller_k, shade = TRUE, pages = 1)
```

